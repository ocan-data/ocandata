{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp statscan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inventory\n",
    "> API details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from ocandata.repo import Repo\n",
    "import logging\n",
    "from chardet import detect\n",
    "\n",
    "logger = logging.getLogger(\"ocandata\")\n",
    "\n",
    "\n",
    "def optimize_statscan(statscan_data: pd.DataFrame):\n",
    "    statscan_data.Element = statscan_data.Element.astype(\"category\")\n",
    "\n",
    "\n",
    "CONTROL_COLS = [\n",
    "    \"VECTOR\",\n",
    "    \"COORDINATE\",\n",
    "    \"DECIMALS\",\n",
    "    \"STATUS\",\n",
    "    \"SYMBOL\",\n",
    "    \"TERMINATED\",\n",
    "    \"SCALAR_FACTOR\",\n",
    "    \"SCALAR_ID\",\n",
    "    \"DGUID\",\n",
    "    \"UOM\",\n",
    "    \"UOM_ID\",\n",
    "]\n",
    "STATSCAN_TYPES = {\n",
    "    \"Age group\": \"category\",\n",
    "    \"Sex\": \"category\",\n",
    "    \"UOM\": \"category\",\n",
    "    \"UOM_ID\": \"category\",\n",
    "    \"GEO\": \"category\",\n",
    "    \"SCALAR_FACTOR\": \"category\",\n",
    "    \"SCALAR_ID\": \"category\",\n",
    "    \"STATUS\": \"category\",\n",
    "    \"SYMBOL\": \"category\",\n",
    "}\n",
    "\n",
    "\n",
    "def read_statscan_csv(statcan_fn: str):\n",
    "    return pd.read_csv(statcan_fn, dtype=STATSCAN_TYPES, low_memory=False)\n",
    "\n",
    "\n",
    "def to_wide_format(statscan_data: pd.DataFrame, pivot_column):\n",
    "    \"\"\"\n",
    "    Converts statscan data to wide format\n",
    "    :param statscan_data:\n",
    "    :return: a dataframe with the statscan data converted to wide format\n",
    "    \"\"\"\n",
    "    base = statscan_data.copy()\n",
    "    group_cols = [\n",
    "        col\n",
    "        for col in base.columns.tolist()\n",
    "        if col not in CONTROL_COLS + [pivot_column, \"VALUE\"]\n",
    "    ]\n",
    "    # Assign a group number\n",
    "    base[\"group\"] = base.groupby(group_cols).ngroup()\n",
    "\n",
    "    # Pivot on the group, turning the element into columns\n",
    "    values = base.pivot_table(\n",
    "        index=\"group\",\n",
    "        columns=pivot_column,\n",
    "        values=\"VALUE\",\n",
    "        aggfunc=np.max,\n",
    "        dropna=False,\n",
    "    )\n",
    "    # Drop Element and VALUE columns and drop duplicates\n",
    "    base = base.drop(columns=[pivot_column, \"VALUE\"]).drop_duplicates(subset=group_cols)\n",
    "    # Now merge with values\n",
    "    return base.merge(values, on=\"group\").drop(columns=\"group\")\n",
    "\n",
    "\n",
    "_STATSCAN_DATASET_RE = re.compile(\"(\\d+)(\\-(eng|fra))?\\.(\\w+)+\")\n",
    "\n",
    "\n",
    "class StatscanUrl:\n",
    "    def __init__(\n",
    "        self,\n",
    "        baseurl: str,\n",
    "        file: str,\n",
    "        resourceid: str,\n",
    "        extension: str,\n",
    "        data: str,\n",
    "        metadata: str,\n",
    "        language: str = None,\n",
    "    ):\n",
    "        self.baseurl = baseurl\n",
    "        self.file = file\n",
    "        self.resourceid = resourceid\n",
    "        self.language = language\n",
    "        self.partitions = [language]\n",
    "        self.extension = extension\n",
    "        self.data = data\n",
    "        self.metadata = metadata\n",
    "\n",
    "    @classmethod\n",
    "    def parse_from_filename(cls, url: str):\n",
    "        filename = os.path.basename(url)\n",
    "        baseurl = url[: url.index(filename)]\n",
    "        match = _STATSCAN_DATASET_RE.match(filename)\n",
    "        if match:\n",
    "            file = match.group(0)\n",
    "            resourceid = match.group(1)\n",
    "            language = match.group(3)\n",
    "            extension = match.group(4)\n",
    "            data = f\"{match.group(1)}.csv\"\n",
    "            metadata = f\"{match.group(1)}_MetaData.csv\"\n",
    "            return StatscanUrl(\n",
    "                baseurl=baseurl,\n",
    "                file=file,\n",
    "                resourceid=resourceid,\n",
    "                extension=extension,\n",
    "                data=data,\n",
    "                metadata=metadata,\n",
    "                language=language,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Does not seem to be a valid statscan dataset url: \" + url)\n",
    "\n",
    "    def id(self):\n",
    "        return f\"{self.baseurl}{self.resourceid}\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"StatscanUrl {self.__dict__}\"\n",
    "\n",
    "\n",
    "statscan_zipurl_re = re.compile(r\".*[0-9]+(\\-(en|fr)\\w+?)?\\.zip.*?\")\n",
    "\n",
    "\n",
    "class StatscanZip(object):\n",
    "    def __init__(self, url: str, repo: Repo = None):\n",
    "        assert statscan_zipurl_re.fullmatch(url)\n",
    "        self.url: str = url\n",
    "        self.url_info: StatscanUrl = StatscanUrl.parse_from_filename(url)\n",
    "        self.repo: Repo = repo or Repo.at_user_home()\n",
    "\n",
    "    def dimensions(self):\n",
    "        return self.get_metadata().dimensions\n",
    "\n",
    "    def primary_dimension(self):\n",
    "        return self.get_metadata().pivot_column()\n",
    "\n",
    "    def get_units_of_measure(self):\n",
    "        return self.units_of_measure\n",
    "\n",
    "    @classmethod\n",
    "    def _apply_dtypes(cls, data: pd.DataFrame):\n",
    "        for col in data:\n",
    "            if col in [\"REF_DATE\"]:\n",
    "                data[col] = pd.to_datetime(data[col]).dt.normalize()\n",
    "\n",
    "    def _fetch_data(self):\n",
    "        resource_id: str = self.url_info.resourceid\n",
    "        data_file, metadata_file = self.repo.unzip(self.url, resource_id=resource_id)\n",
    "        return data_file, metadata_file\n",
    "\n",
    "    def transform_statscan_data(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        wide=True,\n",
    "        index_col: str = None,\n",
    "        drop_control_cols=True,\n",
    "    ):\n",
    "        primary_dimension = self.primary_dimension()\n",
    "        units_of_measure = (\n",
    "            data[[primary_dimension, \"UOM\"]]\n",
    "            .drop_duplicates()\n",
    "            .set_index(primary_dimension)\n",
    "            .sort_index()\n",
    "        )\n",
    "        setattr(self, \"units_of_measure\", units_of_measure)\n",
    "        if wide:\n",
    "            data = to_wide_format(data, pivot_column=self.primary_dimension())\n",
    "        if index_col:\n",
    "            data = data.set_index(index_col)\n",
    "\n",
    "        if drop_control_cols:\n",
    "            drop_cols = [col for col in CONTROL_COLS if col in data.columns]\n",
    "            data = data.drop(columns=drop_cols)\n",
    "\n",
    "        # Convert types\n",
    "        if 'REF_DATE' in data:\n",
    "            if not data['REF_DATE'].isnull().any():\n",
    "                data['REF_DATE'] = pd.to_datetime(data['REF_DATE'])\n",
    "\n",
    "        data = data.rename(columns={'REF_DATE': 'Date', 'GEO':'Geo'})\n",
    "        return data\n",
    "\n",
    "    def _set_metadata(self, metadata_file):\n",
    "        metadata: StatscanMetadata = StatscanMetadata(metadata_file)\n",
    "        setattr(self, \"metadata\", metadata)\n",
    "\n",
    "    def get_metadata(self):\n",
    "        if not hasattr(self, \"metadata\"):\n",
    "            data_file, metadata_file = self._fetch_data()\n",
    "            self._set_metadata(metadata_file)\n",
    "        return self.metadata\n",
    "\n",
    "    def get_data(\n",
    "        self,\n",
    "        wide=True,\n",
    "        index_col: str = None,\n",
    "        drop_control_cols=True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Get the data from this zipfile\n",
    "        :param wide: whether to make this a wide dataset\n",
    "        :param index_col: the column to use as the index\n",
    "        :param drop_control_cols: whether to drop the control columns\n",
    "        :return: a Dataframe containing the data\n",
    "        \"\"\"\n",
    "        if not hasattr(self, \"data\"):\n",
    "            data_file, metadata_file = self._fetch_data()\n",
    "            self._set_metadata(metadata_file)\n",
    "            data_raw = read_statscan_csv(data_file)\n",
    "            data = self.transform_statscan_data(\n",
    "                data_raw,\n",
    "                wide=wide,\n",
    "                index_col=index_col,\n",
    "                drop_control_cols=drop_control_cols,\n",
    "            )\n",
    "            setattr(self, \"data\", data)\n",
    "        return self.data\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"<{self.__class__.__name__}: {self.url}>\"\n",
    "\n",
    "\n",
    "class StatscanMetadata(object):\n",
    "\n",
    "    def __init__(self, metadata_file):\n",
    "        self.sections = StatscanMetadata.parse_metadata(metadata_file)\n",
    "\n",
    "    @property\n",
    "    def notes(self):\n",
    "        return self.sections['Notes']\n",
    "\n",
    "    @property\n",
    "    def cube_info(self):\n",
    "        return self.sections['CubeInfo']\n",
    "\n",
    "    @property\n",
    "    def dimensions(self):\n",
    "        return self.sections['Dimensions']\n",
    "\n",
    "    @classmethod\n",
    "    def parse_sections(cls, metadata_file):\n",
    "        encoding = get_encoding_type(metadata_file)\n",
    "        with open(metadata_file, 'r', encoding=encoding) as f:\n",
    "            start_section = True\n",
    "            for line in f.readlines():\n",
    "                line = line.strip()\n",
    "                if len(line) > 0:\n",
    "                    parts = [p.replace('\"', '') for p in line.split(',')]\n",
    "                    if start_section:\n",
    "                        columns = parts\n",
    "                        rows = []\n",
    "                        start_section = False\n",
    "                    else:\n",
    "                        rows.append(tuple(parts))\n",
    "                else:\n",
    "                    start_section = True\n",
    "                    row_widths = [len(row) for row in rows]\n",
    "                    if row_widths == []:\n",
    "                        continue\n",
    "                    row_width = max(row_widths)\n",
    "                    if row_width > len(columns):\n",
    "                        rows = [row[:len(columns)] for row in rows]\n",
    "                    elif len(columns) > row_width:\n",
    "                        columns = columns[:row_width]\n",
    "                    df = pd.DataFrame(data=rows, columns=columns)\n",
    "                    yield df\n",
    "\n",
    "    @classmethod\n",
    "    def parse_metadata(cls, metadata_file: str):\n",
    "        metadata_sections = cls.parse_sections(metadata_file)\n",
    "        sections = {}\n",
    "        for df in metadata_sections:\n",
    "            if list_contains(df.columns, ['Cube Title', 'Product Id']):\n",
    "                sections['CubeInfo'] = df\n",
    "            elif list_contains(df.columns, ['Dimension ID', 'Dimension name']):\n",
    "                sections['Dimensions'] = df\n",
    "            elif list_contains(df.columns, ['Dimension ID', 'Member Name']):\n",
    "                sections['DimensionValues'] = df\n",
    "            elif list_contains(df.columns, ['Note ID', 'Note']):\n",
    "                sections['Notes'] = df\n",
    "\n",
    "        return sections\n",
    "\n",
    "    def pivot_column(self):\n",
    "        return self.dimensions.tail(1)[\"Dimension name\"].values[0]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"<{self.name}>\"\n",
    "\n",
    "    def _repr_html_(self):\n",
    "        \"\"\"\n",
    "        This is for Jupyter notebooks to automatically display the metadata\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        _html = f\"<h2>{self.name}</h2>\"\n",
    "        _html += self.cube_info._repr_html_()\n",
    "        _html += \"<h3>Dimensions</h3>\"\n",
    "        _html += self.dimensions._repr_html_()\n",
    "        _html += \"<h3>Notes</h3>\"\n",
    "        _html += self.notes._repr_html_()\n",
    "        return _html\n",
    "\n",
    "\n",
    "def get_encoding_type(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        rawdata = f.read()\n",
    "        return detect(rawdata)['encoding'].lower()\n",
    "\n",
    "\n",
    "def list_contains(alist, values):\n",
    "    return all([v in alist for v in values])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_config.ipynb.\n",
      "Converted 01_core.ipynb.\n",
      "Converted 02_inventory.ipynb.\n",
      "Converted 03_repo.ipynb.\n",
      "Converted 04_statscan.ipynb.\n",
      "Converted 05_text.ipynb.\n",
      "Converted 06_datasets.ipynb.\n",
      "Converted 07_datatools.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocandata",
   "language": "python",
   "name": "ocandata"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
